---
weight: 1
bookFlatSection: true
title: "Spring 2024"
---

# Blog-Post-Assignment
This is the Github page for the class, Efficient ML Systems (EECE695D-01).  Students (will) upload a blog post reviewing the paper.

## Guideline for Students
* First, make the folder named `Paper Number_Paper Title_Member1_Member2 (e.g. 0_TACO_HagyeongLee)`. You can use initials of paper title. If there is no optimal initials, then you can skip the paper name (e.g. 0_HagyeongLee).
* Second, you put the markdown file and assests(e.g. figures) in the folder.
* Finally, if you verify that the markdown file is rendering successfully.


## Posts

| Number 	| Title 	| Team 	| Blog Post LINK 	|
|:---:|:---:|:---:|:---:|
|  	|  	| Member 1, Member 2 	|  	|
| 0 	| **_(example)_** [Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity](https://arxiv.org/abs/2403.02944) 	| Hagyeong Lee 	| [LINK]() 	|
| 1 	| [Is Bigger Edit Batch Size Always Better? -- An Empirical Study on Model Editing with Llama-3         	](https//arxiv.org/abs/2405.00666) 	| Jin Hyun, Gyuhyun Jung 	|  	|
| 2 	| [Spectrally Pruned Gaussian Fields with Neural Compensation                          	](https//arxiv.org/pdf/2405.00676) 	| Donggeon Lee, Chiho yoon 	|  	|
| 3 	| [Unit Scaling: Out-of-the-Box Low-Precision Training                              	](https//arxiv.org/abs/2303.11257) 	| SeongRok Moon, Changyoung Ju 	|  	|
| 4 	| [Better & Faster Large Language Models via Multi-token Prediction                       	](https//arxiv.org/abs/2404.19737) 	| Jinoh Cho, Seonghyeon Park 	|  	|
| 5 	| [Lossless Self-Speculative Decoding via Double Early Exiting                          	](https//arxiv.org/abs/2404.18911) 	| Nayoung Kwon, Jiwoong Im 	|  	|
| 6 	| [XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference                    	](https//arxiv.org/abs/2404.15420) 	| Hyundong Kim, Sangil Han 	|  	|
| 7 	| [VeRA: Vector-based Random Matrix Adaptation                                  	](https//arxiv.org/abs/2310.11454) 	| Kyumin Cho, Sejin Park 	|  	|
| 8 	| [Mixture of LoRA Experts                                            	](https//arxiv.org/abs/2404.13628) 	| Jegwang Ryu, Sangbeom Ha 	|  	|
| 9 	| [MobileNetV4 -- Universal Models for the Mobile Ecosystem                           	](https//arxiv.org/abs/2404.10518) 	| JoonSeok Kim, DongGyu Kim 	|  	|
| 10 	| [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length               	](https//arxiv.org/pdf/2404.08801) 	| Hyunho Kook 	|  	|
| 11 	| [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention            	](https//arxiv.org/abs/2404.07143) 	| Younghyun Cho, Sangjun Lee 	|  	|
| 12 	| [Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies         	](https//arxiv.org/abs/2404.08197) 	| Junkyeong Park, Harit Keawmuang 	|  	|
| 13 	| [A Large-Scale Exploration of Î¼-Transfer                                    	](https//arxiv.org/abs/2404.05728) 	| Jeonghyun Choi, Minhye Choo 	|  	|
| 14 	| [BinaryDM: Towards Accurate Binarization of Diffusion Model                          	](https//arxiv.org/abs/2404.05662) 	| Junhyuk So, Juncheol Shin 	|  	|
| 15 	| [Training LLMs over Neurally Compressed Text                                  	](https//arxiv.org/abs/2404.03626) 	| Seonghyun Park, Jiwoo Kim 	|  	|
| 16 	| [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models            	](https//arxiv.org/abs/2404.02258) 	| Minjae Park, Inkwan Hwang 	|  	|
| 17 	| [QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs                             	](https//arxiv.org/abs/2404.00456) 	| MyeongJi Yun, Jung Gyu Min 	|  	|
| 18 	| [ViTAR: Vision Transformer with Any Resolution                                 	](https//arxiv.org/abs/2403.18361) 	| Jungwon Lee, Minsang Seok 	|  	|
| 19 	| [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning           	](https//arxiv.org/abs/2403.17919) 	| Sungbin Shin, Dongyeop Lee 	|  	|
| 20 	| [Evolutionary Optimization of Model Merging Recipes                              	](https//arxiv.org/abs/2403.13187) 	| Youngkil Song, Jaehyeon Park 	|  	|
| 21 	| [A Unified Framework for Model Editing                                     	](https//arxiv.org/abs/2403.14236) 	| Jonghyun Chae, Donggeun An 	|  	|
| 22 	| [Larimar: Large Language Models with Episodic Memory Control                          	](https//arxiv.org/abs/2403.11901) 	| Sunggyu Jang, Hyeonwoo Park 	|  	|
| 23 	| [Beyond Language Models: Byte Models are Digital World Simulators                       	](https//arxiv.org/abs/2402.19155) 	| Dohun Kim, Yeongwoo Kim 	|  	|
| 24 	| [LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression          	](https//arxiv.org/abs/2403.12968) 	| Seungjoo Shin, Sua Choi 	|  	|
| 25 	| [Merging Text Transformer Models from Different Initializations                        	](https//arxiv.org/abs/2403.00986) 	| Minwoo Kim, Kyungtae Kim 	|  	|


# Contact
If you have any questions, please feel free to contact TA (hagyeonglee@postech.ac.kr).